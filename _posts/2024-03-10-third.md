---
layout: single
title: "Tracing kthread_create"
categories: Linux_kernel
toc: true
author_profile: true
tag: [Linux, kernel, Kthread, tracing]
share: false
---

## /rpi_kernel_src/linux/include/linux # vi kthread.h
``` c
27 #define kthread_create(threadfn, data, namefmt, arg...) \
28 	kthread_create_on_nede(threadfn, data, NUMA_NO_NODE, namefmt, ##arg)
```

>threadfn : function handler, function pointer
>data : struct
>namefmt : string for naming a thread
>arg... : a variadic macro that accepts format specifiers included in namefmt. It is used to format the name of the thread.

>include/linux/numa.h : 14 # define NUMA_NO_NODE (-1)

```c
11 struct task_struct *kthread_create_on_node(int (*threadfn)(void *data), 
                                            void *data, 
                                            int node, 
                                            const char namefmt[], 
                                            ...
                                            );
```


> (*threadfn)(void *data) : field for storing the address of handler function, which defines detailed behaviours of the thread
> void *data : parameters passed to the handler function, typically passing the address of a structure
> include/linux/numa.h : 14 # define NUMA_NO_NODE (-1) : set -1 as a default value
> const char namefmt[] : storing the name of the kernel thread

## /rpi_kernel_src/linux/kernel # vi kthread.c
```c
 480 /**
 481  * kthread_create_on_node - create a kthread.
 482  * @threadfn: the function to run until signal_pending(current).
 483  * @data: data ptr for @threadfn.
 484  * @node: task and thread structures for the thread are allocated on this node
 485  * @namefmt: printf-style name for the thread.
 486  *
 487  * Description: This helper function creates and names a kernel
 488  * thread.  The thread will be stopped: use wake_up_process() to start
 489  * it.  See also kthread_run().  The new thread has SCHED_NORMAL policy and
 490  * is affine to all CPUs.
 491  *
 492  * If thread is going to be bound on a particular cpu, give its node
 493  * in @node, to get NUMA affinity for kthread stack, or else give NUMA_NO_NODE.
 494  * When woken, the thread will run @threadfn() with @data as its
 495  * argument. @threadfn() can either return directly if it is a
 496  * standalone thread for which no one will call kthread_stop(), or
 497  * return when 'kthread_should_stop()' is true (which means
 498  * kthread_stop() has been called).  The return value should be zero
 499  * or a negative error number; it will be passed to kthread_stop().
 500  *
 501  * Returns a task_struct or ERR_PTR(-ENOMEM) or ERR_PTR(-EINTR).
 502  */
 503 struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 504                                            void *data, int node,
 505                                            const char namefmt[],
 506                                            ...)
 507 {
 508         struct task_struct *task;
 509         va_list args;
 510
 511         va_start(args, namefmt);
 512         task = __kthread_create_on_node(threadfn, data, node, namefmt, args);
 513         va_end(args);
 514
 515         return task;
 516 }
 517 EXPORT_SYMBOL(kthread_create_on_node);
 518
```

> va_start(args, namegmt) : to process the variable arguments
> va_end(args) : clean the variable arguments to prevent memory leaks by reclaiming resources

```c
 414 struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
 415                                                     void *data, int node,
 416                                                     const char namefmt[],
 417                                                     va_list args)
 418 {
 419         DECLARE_COMPLETION_ONSTACK(done);
 420         struct task_struct *task;
 421         struct kthread_create_info *create = kmalloc(sizeof(*create),
 422                                                      GFP_KERNEL);
 // making struct to store data for the new thread
 423
 424         if (!create)
 425                 return ERR_PTR(-ENOMEM);
 426         create->threadfn = threadfn;
 427         create->data = data;
 428         create->node = node;
 429         create->done = &done;
 430
 431         spin_lock(&kthread_create_lock); // synchronization techniques
 432         list_add_tail(&create->list, &kthread_create_list); // add to list: after this adding, kthreadd process will make a kernel process for this
 433         spin_unlock(&kthread_create_lock); // and then unlock
 434
 435         wake_up_process(kthreadd_task);
 436         /*
 437          * Wait for completion in killable state, for I might be chosen by
 438          * the OOM killer while kthreadd is trying to allocate memory for
 439          * new kernel thread.
 440          */
 441         if (unlikely(wait_for_completion_killable(&done))) {
 442                 /*
 443                  * If I was killed by a fatal signal before kthreadd (or new
 444                  * kernel thread) calls complete(), leave the cleanup of this
 445                  * structure to that thread.
 446                  */
 447                 if (xchg(&create->done, NULL))
 448                         return ERR_PTR(-EINTR);
 449                 /*
 450                  * kthreadd (or new kernel thread) will call complete()
 451                  * shortly.
 452                  */
 453                 wait_for_completion(&done);
 454         }
 455         task = create->result;
 456         if (!IS_ERR(task)) {
 457                 char name[TASK_COMM_LEN];
 458                 va_list aq;
 459                 int len;
 460
 461                 /*
 462                  * task is already visible to other tasks, so updating
 463                  * COMM must be protected.
 464                  */
 465                 va_copy(aq, args);
 466                 len = vsnprintf(name, sizeof(name), namefmt, aq);
 467                 va_end(aq);
 468                 if (len >= TASK_COMM_LEN) {
 469                         struct kthread *kthread = to_kthread(task);
 470
 471                         /* leave it truncated when out of memory. */
 472                         kthread->full_name = kvasprintf(GFP_KERNEL, namefmt, args);
 473                 }
 474                 set_task_comm(task, name);
 475         }
 476         kfree(create);
 477         return task;
 478 }
```

> wake_up_process(kthreadd_task);
> : if kthreadd process is waken up, kthreadd() which the handler function of kthreadd process works

```c
 718 int kthreadd(void *unused)
 719 {
 720         struct task_struct *tsk = current;
 721
 722         /* Setup a clean context for our children to inherit. */
 723         set_task_comm(tsk, "kthreadd");
 724         ignore_signals(tsk);
 725         set_cpus_allowed_ptr(tsk, housekeeping_cpumask(HK_TYPE_KTHREAD));
 726         set_mems_allowed(node_states[N_MEMORY]);
 727
 728         current->flags |= PF_NOFREEZE;
 729         cgroup_init_kthreadd();
 730
 731         for (;;) {
 732                 set_current_state(TASK_INTERRUPTIBLE);
 733                 if (list_empty(&kthread_create_list))
 734                         schedule();
 // if the list is empty, put the current executing thread into the wait queue 
 // and yield the CPU to allow the scheduler to execute the next scheduled thread
 735                 __set_current_state(TASK_RUNNING);  // else, running
 736
 737                 spin_lock(&kthread_create_lock);  // lock
 738                 while (!list_empty(&kthread_create_list)) {
 739                         struct kthread_create_info *create;
 740
 741                         create = list_entry(kthread_create_list.next,
 742                                             struct kthread_create_info, list);
 // list_entry is a macro function, 
 // extracting a pointer to a struct kthread_create_info from the kthread_create_list linked list.
 743                         list_del_init(&create->list);
 744                         spin_unlock(&kthread_create_lock);
 745
 746                         create_kthread(create); // create thread
 747
 748                         spin_lock(&kthread_create_lock);
 749                 }
 750                 spin_unlock(&kthread_create_lock);
 751         }
 752
 753         return 0;
 754 }
 755
 ```

 ```c
 35 static LIST_HEAD(kthread_create_list);
 ```

## /rpi_kernel_src/linux/include/linux # vi list.h
```c
23 #define LIST_HEAD_INIT(name) { &(name), &(name)}
24
25 #define LIST_HEAD(name) \
26      struct list_head name = LIST_HEAD_INIT(name)
```

## /rpi_kernel_src/linux/include/linux # vi types.h
```c
178 struct list_head {
    struct list_head *next, *prev;
};
```


## /rpi_kernel_src/linux/kernel # vi kthread.c
```c
  38 struct kthread_create_info
  39 {
  40         /* Information passed to kthread() from kthreadd. */
  41         int (*threadfn)(void *data);
  42         void *data;
  43         int node;
  44
  45         /* Result passed back to kthread_create() from kthreadd. */
  46         struct task_struct *result;
  47         struct completion *done;
  48
  49         struct list_head list;
  50 };
```

```c
 391 static void create_kthread(struct kthread_create_info *create)
 392 {
 393         int pid;
 394
 395 #ifdef CONFIG_NUMA
 396         current->pref_node_fork = create->node;
 397 #endif
 398         /* We want our own signal handler (we take no signals by default). */
 399         pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
 400         if (pid < 0) {
 401                 /* Release the structure when caller killed by a fatal signal. */
 402                 struct completion *done = xchg(&create->done, NULL);
 403
 404                 if (!done) {
 405                         kfree(create);
 406                         return;
 407                 }
 408                 create->result = ERR_PTR(pid);
 409                 complete(done);
 410         }
 411 }
```

## /rpi_kernel_src/linux/kernel # vi fork.c
```c
2261 /*
2262  * Create a kernel thread.
2263  */
2264 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
2265 {
2266         return _do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
2267                 (unsigned long)arg, NULL, NULL, 0);
2268 }
```

> _do_fork() updated to kernel_clone()

```c
2170 /*
2171  *  Ok, this is the main fork-routine.
2172  *
2173  * It copies the process, and if successful kick-starts
2174  * it and waits for it to finish using the VM if required.
2175  */
2176 long _do_fork(unsigned long clone_flags,
2177               unsigned long stack_start,
2178               unsigned long stack_size,
2179               int __user *parent_tidptr,
2180               int __user *child_tidptr,
2181               unsigned long tls)
2182 {
2183         struct completion vfork;
2184         struct pid *pid;
2185         struct task_struct *p;
2186         int trace = 0;
2189         /*
2193          * for the type of forking is enabled.
2194          */
2195         if (!(clone_flags & CLONE_UNTRACED)) {
2196                 if (clone_flags & CLONE_VFORK)
2197                         trace = PTRACE_EVENT_VFORK;
2198                 else if ((clone_flags & CSIGNAL) != SIGCHLD)
2199                         trace = PTRACE_EVENT_CLONE;
2200                 else
2201                         trace = PTRACE_EVENT_FORK;
2202 
2203                 if (likely(!ptrace_event_enabled(current, trace)))
2204                         trace = 0;
2205         }
2206 
2207         p = copy_process(clone_flags, stack_start, stack_size,
2208                          child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
2209         add_latent_entropy();
2210 
2211         if (IS_ERR(p))
2212                 return PTR_ERR(p);
2213 
2214         /*
2215          * Do this prior waking up the new thread - the thread pointer
2216          * might get invalid after that point, if the thread exits quickly.
2217          */
2218         trace_sched_process_fork(current, p);
2219 
2220         pid = get_task_pid(p, PIDTYPE_PID);
2221         nr = pid_vnr(pid);
2222 
2223         if (clone_flags & CLONE_PARENT_SETTID)
2224                 put_user(nr, parent_tidptr);
2225 
2226         if (clone_flags & CLONE_VFORK) {
2227                 p->vfork_done = &vfork;
2228                 init_completion(&vfork);
2229                 get_task_struct(p);
2230         }
2231 
2232         wake_up_new_task(p);
2233 
2234         /* forking complete and child started to run, tell ptracer */
2235         if (unlikely(trace))
2236                 ptrace_event_pid(trace, pid);
2237 
2238         if (clone_flags & CLONE_VFORK) {
2239                 if (!wait_for_vfork_done(p, &vfork))
2240                         ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
2241         }
2242 
2243         put_pid(pid);
2244         return nr;
2245 }
2246 
2247 #ifndef CONFIG_HAVE_COPY_THREAD_TLS
2248 /* For compatibility with architectures that call do_fork directly rather than
2249  * using the syscall entry points below. */
2250 long do_fork(unsigned long clone_flags,
2251               unsigned long stack_start,
2252               unsigned long stack_size,
2253               int __user *parent_tidptr,
2254               int __user *child_tidptr)
2255 {
2256         return _do_fork(clone_flags, stack_start, stack_size,
2257                         parent_tidptr, child_tidptr, 0);
2258 }
2259 #endif
```
```c
1650 /*
1651  * This creates a new process as a copy of the old one,
1652  * but does not actually start it yet.
1653  *
1654  * It copies the registers, and all the appropriate
1655  * parts of the process environment (as per the clone
1656  * flags). The actual kick-off is left to the caller.
1657  */
1658 static __latent_entropy struct task_struct *copy_process(
1659                                         unsigned long clone_flags,
1663                                         struct pid *pid,
1664                                         int trace,
1665                                         unsigned long tls,
1666                                         int node)
1667 {
1668         int retval;
1669         struct task_struct *p;
1670         struct multiprocess_signals delayed;
1671 
            .........
1735 
1736         retval = -ENOMEM;
1737         p = dup_task_struct(current, node);
1738         if (!p)
1739                 goto fork_out;
1742          * This _must_ happen before we call free_task(), i.e. before we jump
1743          * to any of the bad_fork_* labels. This is to avoid freeing
1744          * p->set_child_tid which is (ab)used as a kthread's data pointer for
1745          * kernel threads (PF_KTHREAD).
            ..........
1865 
1866         /* Perform scheduler related setup. Assign this task to a CPU. */
1867         retval = sched_fork(clone_flags, p);
1868         if (retval)
1869                 goto bad_fork_cleanup_policy;
1870 
1871         retval = perf_event_init_task(p);
1872         if (retval)
1881                 goto bad_fork_cleanup_audit;
1885         retval = copy_files(clone_flags, p);
1886         if (retval)
1889         if (retval)
1890                 goto bad_fork_cleanup_files;
1891         retval = copy_sighand(clone_flags, p);
1894         retval = copy_signal(clone_flags, p);
1895         if (retval)
1898         if (retval)
1899                 goto bad_fork_cleanup_signal;
1900         retval = copy_namespaces(clone_flags, p);
1901         if (retval)
1902                 goto bad_fork_cleanup_mm;
1903         retval = copy_io(clone_flags, p);
1904         if (retval)
1905                 goto bad_fork_cleanup_namespaces;
1906         retval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);
1907         if (retval)
1908                 goto bad_fork_cleanup_io;
1909 
1910         if (pid != &init_struct_pid) {
1911                 pid = alloc_pid(p->nsproxy->pid_ns_for_children);
1912                 if (IS_ERR(pid)) {
1913                         retval = PTR_ERR(pid);
1914                         goto bad_fork_cleanup_thread;
1915                 }
1916         }
1917 
            ..........
2145 }
```

## /rpi_kernel_src/linux/kernel/sched # vi core.c
```c
2388 /*
2389  * wake_up_new_task - wake up a newly created task for the first time.
2390  *
2391  * This function will do some initial scheduler statistics housekeeping
2394  */
2395 void wake_up_new_task(struct task_struct *p)
2396 {
2403         /*
2411         p->recent_used_cpu = task_cpu(p);
2414         rq = __task_rq_lock(p, &rf);
2415         update_rq_clock(rq);
2416         post_init_entity_util_avg(&p->se);
2417 
2418         activate_task(rq, p, ENQUEUE_NOCLOCK);
2419         p->on_rq = TASK_ON_RQ_QUEUED;
2420         trace_sched_wakeup_new(p);
2421         check_preempt_curr(rq, p, WF_FORK);
2422 #ifdef CONFIG_SMP
2423         if (p->sched_class->task_woken) {
2424                 /*
2425                  * Nothing relies on rq->lock after this, so its fine to
2426                  * drop it.
2427                  */
2428                 rq_unpin_lock(rq, &rf);
2429                 p->sched_class->task_woken(rq, p);
2430                 rq_repin_lock(rq, &rf);
2431         }
2432 #endif
2433         task_rq_unlock(rq, p, &rf);
2434 }
```


## User Process Creation
> fork()
> sys_clone : system call
> _do_fork() or kernel_clone()
> copy_process()
> wake_up_new_task()

## Kernel Process Creation
> kthread_create() = macro calling => kthread_create_on_node() 
> __kthread_create_on_node() : create and wake up new thread 
> kthreadd()
> create_kthread()
> kernel_thread()
> _do_fork() or kernel_clone()
> copy_process()
> wake_up_new_task()